{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cebb0966",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "\n",
    "Consider a dataset with more potentially useful features which can be used for prediction as shown below,\n",
    "\n",
    "| Feature 1 $$x_1$$ | Feature 2 $$x_2$$ | Feature 3 $$x_3$$ | Feature 4 $$x_4$$ | Target Variable $$y$$ |\n",
    "|---|---|---|---|---|\n",
    "|2104|5|1|45|460|\n",
    "|1416|3|2|40|232|\n",
    "|1534|3|2|30|315|\n",
    "|852|2|1|36|178|\n",
    "|$$\\ldots$$|$$\\ldots$$|$$\\ldots$$|$$\\ldots$$|$$\\ldots$$|\n",
    "\n",
    "This can now represent a multivariate linear regression problem, where $x^{(i)}\\in\\mathbb{R}^n$, for $n$ features with $i\\in[1,\\ldots,m]$. In this particular case, $n=4$.\n",
    "\n",
    "The $j$-th input feature, where $j\\in[1,\\ldots,n]$. for the $i$-th training example, where $i\\in[1,\\ldots,m]$, shall be denoted as $x_j^{(i)}$\n",
    "\n",
    "The model for linear regression can now be expressed as,\n",
    "$$\n",
    "f(x^{(i)})=w_1x_1^{(i)}+w_2x_2^{(i)}+\\dots+w_jx_j^{(i)}+b\n",
    "$$\n",
    "\n",
    "A more compact notation uses vectors to represent $w\\in\\mathbb{R}^n$ and $x^{(i)}$, with $b\\in\\mathbb{R}$, such that\n",
    "$$\n",
    "w=\\begin{bmatrix}w_1\\\\\\vdots\\\\w_j\\end{bmatrix}\\quad x^{(i)}=\\begin{bmatrix}x_1^{(i)}\\\\\\vdots\\\\x_j^{(i)}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the linear regression model can be rewritten as,\n",
    "$$\n",
    "f(x^{(i)})=w^\\top x^{(i)}+b\n",
    "$$\n",
    "\n",
    "## Vectorized Implementation\n",
    "An implementation which is much more computationally efficient can be achieved through *vectorization*, i.e, using the parallel processing capabilities of modern processors (CPU or GPU) to compute linear algebra calculations much faster than through loops.\n",
    "\n",
    "Let $X\\in\\mathbb{R}^{n\\times m}$ be defined as the *design matrix* contaning all values of $x_j^{(i)}$ for all $i\\in[1,\\ldots,m]$ and $j\\in[1,\\ldots,n]$, and let $Y$ be defined as a row vector containing all values of $y^{(i)}$\n",
    "$$\n",
    "    X=\n",
    "    \\begin{bmatrix}\n",
    "        x^{(1)}_1 & \\dots  & x^{(m)}_1 \\\\\n",
    "        \\vdots    & \\ddots & \\vdots    \\\\\n",
    "        x^{(1)}_n & \\dots  & x^{(m)}_n\n",
    "    \\end{bmatrix}\n",
    "    \\qquad\n",
    "    Y=\\begin{bmatrix}y^{(1)}& \\dots &y^{(m)}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Given parameters $w\\in\\mathbb{R}^{n}$ and $b\\in\\mathbb{R}$, the vector containing all predicted values $\\hat{Y}$ can be achieved with very little changes from the original equations,\n",
    "$$\n",
    "    \\hat{Y}=w^\\top X+b\n",
    "$$\n",
    "\n",
    "With that said, the variable $b$ does not necessarily need to be a separate variable. Including a *dummy* feature $x^{(i)}_{j+1}=1$ for all $i$, within the design matrix allows for an additional component to $w$ as follows,\n",
    "$$\n",
    "X=\n",
    "    \\begin{bmatrix}\n",
    "        x^{(1)}_1 & \\dots  & x^{(m)}_1 \\\\\n",
    "        \\vdots    & \\ddots & \\vdots    \\\\\n",
    "        x^{(1)}_n & \\dots  & x^{(m)}_n \\\\\n",
    "        1 & \\dots & 1\n",
    "    \\end{bmatrix}\n",
    "    \\qquad\n",
    "    w=\\begin{bmatrix}w_1\\\\\\vdots\\\\w_n\\\\b\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With this, the calculations for $\\nabla_{w}J(w)$ remain equivalent (see gradient descent), and the equation for $\\hat{Y}$ becomes\n",
    "$$\n",
    "    \\hat{Y}=w^\\top X\n",
    "$$\n",
    "\n",
    "## Gradient Descent\n",
    "Given the cost function $J(w,b)$ which has been previously defined as the *mean squared error*,\n",
    "$$\n",
    "J(w,b)=\\frac{1}{2m}\\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^2\n",
    "$$\n",
    "The gradient of the cost function w.r.t the weights $w$ is\n",
    "$$\n",
    "\\nabla_{\\vec{w}}J(w,b)=\\begin{bmatrix}\n",
    "    \\frac{\\partial J(w,b)}{\\partial w_1}, & \\dots &, \\frac{\\partial J(w,b)}{\\partial w_j}\n",
    "\\end{bmatrix}^\\top\n",
    "$$\n",
    "\n",
    "The components of this gradient can be calculated as,\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J(w,b)}{\\partial w_j}\n",
    "    &=\\frac{\\partial}{\\partial w_j}\\left[\\frac{1}{2m}\\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^2\\right]\\\\\n",
    "    &=\\frac{1}{2m}\\cdot\\sum_{i=1}^{m}\\left[\\frac{\\partial}{\\partial w_j}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)^2\\right]\\\\\n",
    "    &=\\frac{1}{2m}\\cdot\\sum_{i=1}^{m}\\left[2\\left(\\hat{y}^{(i)}-y^{(i)}\\right)\\cdot\\frac{\\partial}{\\partial w_j}\\left(w^\\top x^{(i)}+b-y^{(i)}\\right)\\right]\\\\\n",
    "    &=\\frac{1}{2m}\\sum_{i=1}^{m}\\left[2\\left(\\hat{y}^{(i)}-y^{(i)}\\right)\\cdot x^{(i)}_j\\right]\\\\\n",
    "    &=\\frac{1}{m}\\sum_{i=1}^{m}(\\hat{y}^{(i)}-y^{(i)})\\cdot x^{(i)}_j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Likewise, $\\frac{\\partial J(w,b)}{b}$ can be calculated as \n",
    "$$\n",
    "\\frac{\\partial J(w,b)}{\\partial b}=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)\n",
    "$$\n",
    "\n",
    "Given the design matrix $X$ and parameters $w$ (with $b$ included), a vectorized implementation of $\\nabla_{w}J(w)$ can be calculated as follows:\n",
    "\n",
    "The residuals $\\hat{Y}-Y$ can be obtained simply by subtracting the two vectors. After which, the vector can be transposed leading to,\n",
    "$$\n",
    "    (\\hat{Y}-Y)^\\top=\\begin{bmatrix}\\hat{y}^{(1)}-y^{(1)}\\\\\\vdots\\\\\\hat{y}^{(m)}-y^{(m)}\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Recall that the design matrix $X$ is defined as\n",
    "$$\n",
    "X=\n",
    "    \\begin{bmatrix}\n",
    "        x^{(1)}_1 & \\dots  & x^{(m)}_1 \\\\\n",
    "        \\vdots    & \\ddots & \\vdots    \\\\\n",
    "        x^{(1)}_n & \\dots  & x^{(m)}_n \\\\\n",
    "        1 & \\dots & 1\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With this, the gradient $\\nabla_{w}J(w)$ can simply be calculated as a matrix-vector product,\n",
    "$$\n",
    "    \\nabla_{w}J(w)=\\frac{X(\\hat{Y}-Y)^\\top}{m}\n",
    "$$\n",
    "\n",
    "To see that this is valid, note that each component, $\\nabla_{w}J(w)_j$ for $j\\in[1,\\ldots,n]$ evaluates to\n",
    "$$\n",
    "\\nabla_{w}J(w)_j=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{y}^{(i)}-y^{(i)}\\right)\\cdot x^{(i)}_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0052ec1",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2758ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a877e9",
   "metadata": {},
   "source": [
    "Loading the **diabetes** dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b664147",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_X, diabetes_Y = datasets.load_diabetes(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7a2b21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "512a480a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee89d4d5",
   "metadata": {},
   "source": [
    "Transforming the datasets into the proper matrixes for $X$ and $Y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818bd053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 442), (1, 442))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabetes_X = diabetes_X.T\n",
    "diabetes_X = np.row_stack([diabetes_X, np.full((1, diabetes_X.shape[1]), 1)])\n",
    "diabetes_Y = diabetes_Y[:, np.newaxis].T\n",
    "\n",
    "diabetes_X.shape, diabetes_Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61318f",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "To improve the performance of gradient descent, it helps to scale the features such that they are similar or comparable in terms of their ranges. There are multiple ways to perform these transformation: **z-score normalization**, or **mean normalization**\n",
    "\n",
    "For *z-score normalization*, given the mean $\\mu_j$ and standard deviation $\\sigma_j$ for every feature $j\\in[1,\\ldots,n]$, the input feature $x^{(i)}_j$ for $i\\in[1,\\ldots,m]$ is transformed as\n",
    "$$\n",
    "x^{(i)}_j\\leftarrow\\frac{x^{(i)}_j-\\mu_j}{\\sigma_j}\n",
    "$$\n",
    "thus the resulting features now have $\\mu_j=0$ and $\\sigma_j=1$.\n",
    "\n",
    "For *mean normalization*, given the mean $\\mu_j$ and range $x_{j,max}-x_{j,min}$ for every feature $j\\in[1,\\ldots,n]$, the input feature $x^{(i)}_j$ for $i\\in[1,\\ldots,m]$ is transformed as\n",
    "$$\n",
    "x^{(i)}_j\\leftarrow\\frac{x^{(i)}_j-\\mu_j}{x_{j,max}-x_{j,min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c79b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_optimal_params_grad_descent(train_X, train_Y, num_epochs, learning_rate):    \n",
    "    n, m = train_X.shape\n",
    "    w    = np.random.randn(n, 1)\n",
    "    \n",
    "    loss_hist = np.zeros(num_epochs)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        Y_pred = np.dot(w.T, train_X)\n",
    "        J_cost = np.mean((Y_pred - train_Y) ** 2) / 2.0\n",
    "        loss_hist[i] = J_cost\n",
    "        \n",
    "        # Calculate gradients\n",
    "        grad_W = np.dot(train_X, (Y_pred - train_Y).T) / m\n",
    "        \n",
    "        # Update the weights\n",
    "        w = w - (learning_rate * grad_W)\n",
    "        \n",
    "    return w, loss_hist\n",
    "\n",
    "weights, loss_hist = calc_optimal_params_grad_descent(diabetes_X, diabetes_Y, 10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1364c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.title('Learning curve for gradient descent')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mean squared error (MSE)')\n",
    "plt.plot(loss_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d83baaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -7.0341685 ],\n",
       "       [-236.60114314],\n",
       "       [ 527.60323623],\n",
       "       [ 321.83339165],\n",
       "       [-163.94305623],\n",
       "       [ -26.00582044],\n",
       "       [-174.92850824],\n",
       "       [ 105.19736381],\n",
       "       [ 514.96833878],\n",
       "       [  69.77383282],\n",
       "       [ 152.13348416]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be965f0b",
   "metadata": {},
   "source": [
    "## The Normal Equation\n",
    "\n",
    "Aside from gradient descent, there is also a direct or analytical method for calculating parameters $w$ for multiple linear regression. The equation for this is known as the *normal equation*. The normal equation for calculating the optimal parameters $w^*$ is given by,\n",
    "\n",
    "$$\n",
    "w^*=\\left(XX^\\top\\right)^{-1}XY^\\top\n",
    "$$\n",
    "\n",
    "*Proof*:\n",
    "\n",
    "As before, the goal is to find parameters $w\\in\\mathbb{R}^{n\\times 1}$ which minimizes the mean squared error across all training examples $Y\\in\\mathbb{R}^{1\\times m}$ given $X\\in\\mathbb{R}^{n\\times m}$, in a multivariable scenario, this is expressed as $J(w)$,\n",
    "$$\n",
    "\\begin{align*}\n",
    "J(w)\n",
    "&=\\frac{1}{2m}\\left(w^\\top X-Y\\right)^\\top\\left(w^\\top X-Y\\right)\\\\\n",
    "&=\\frac{1}{2m}\\left[\\left(w^\\top X\\right)^\\top-Y^\\top\\right]\\left(w^\\top X-Y\\right)\\\\\n",
    "&=\\frac{1}{2m}\\left[(w^\\top X)(w^\\top X)^\\top-w^\\top XY^\\top -Y(w^\\top X)^\\top +Y^\\top Y\\right]\\\\\n",
    "&=\\frac{1}{2m}\\left[w^\\top XX^\\top w-2Y\\left(w^\\top X\\right)^\\top+Y^\\top Y\\right]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We must then calculate the gradient of the cost w.r.t the weights $w$ as follows\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(w)}{\\partial w}\n",
    "&=\\frac{1}{2m}\\cdot\\frac{\\partial}{\\partial w}\\left[w^\\top XX^\\top w-2Y\\left(w^\\top X\\right)^\\top+Y^\\top Y\\right]\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We shall derive each term separately for simplification. Starting with the third term.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}Y^\\top Y=0\n",
    "$$\n",
    "\n",
    "For the second term, recall the structure of both vectors,\n",
    "$$\n",
    "Y=\\begin{bmatrix}y^{(1)} & \\dots & y^{(m)}\\end{bmatrix}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "\\left(w^\\top X\\right)^\\top=\\begin{bmatrix}\n",
    "    w_1x^{(1)}_1+\\ldots+w_nx^{(1)}_n \\\\\n",
    "    \\vdots \\\\\n",
    "    w_1x^{(m)}_1+\\ldots+w_nx^{(m)}_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, the dot product is\n",
    "$$\n",
    "Y\\left(w^\\top X\\right)^\\top\n",
    "=y^{(1)}\\left(w_1x^{(1)}_1+\\ldots+w_nx^{(1)}_n\\right)+\\ldots+y^{(m)}\\left(w_1x^{(m)}_1+\\ldots+w_nx^{(m)}_n\\right)\n",
    "$$\n",
    "\n",
    "Thus the partial derivative w.r.t a weight $w_j$ would be defined as\n",
    "$$\n",
    "\\frac{\\partial }{w_j}Y\\left(w^\\top X\\right)^\\top=\\sum_{i=1}^{m}y^{(i)}x^{(i)}_{j}\n",
    "$$\n",
    "\n",
    "This can be expressed as a matrix vector multiplication\n",
    "$$\n",
    "\\frac{\\partial }{w_j}2Y\\left(w^\\top X\\right)^\\top=2XY^\\top\n",
    "$$\n",
    "\n",
    "To see that this is valid, note that each element in this vector result is,\n",
    "$$\n",
    "(XY^\\top)_{j}=\\sum_{i=1}^{m}y^{(i)}x^{(i)}_j\n",
    "$$\n",
    "\n",
    "To simplify the calculation of the first term, note that by the associativity of matrix multiplication, we can multiply $XX^\\top\\in\\mathbb{R}^{n\\times n}$ to obtain a square and symmetric matrix, further denoted as $A$ where\n",
    "$$\n",
    "A_{j,k}=\\left(XX^\\top\\right)_{j,k}=\\sum_{i=1}^{m}X_{j,i}X^\\top_{i,k}\n",
    "$$\n",
    "\n",
    "Note that $XX^\\top w$ becomes\n",
    "$$\n",
    "XX^\\top w=Aw=\\begin{bmatrix}\n",
    "    w_1A_{1,1}+\\ldots+w_nA_{1,n} \\\\\n",
    "    \\vdots \\\\\n",
    "    w_1A_{n,1}+\\ldots+w_nA_{n,n} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Thus the full product, $w^\\top XX^\\top w$, becomes\n",
    "$$\n",
    "w^\\top XX^\\top w=w^\\top Aw=w_1\\left(w_1A_{1,1}+\\ldots+w_nA_{1,n}\\right)+\\dots+w_n\\left(w_1A_{n,1}+\\ldots+w_nA_{n,n}\\right)\n",
    "$$\n",
    "\n",
    "Calculating the partial derivative w.r.t a weight $w_1$ for example becomes the following, note that $A$ is symmetric, therefore an element such as $A_{2,1}=A_{1,2}$\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial w^\\top Aw}{w_1}\n",
    "&=2w_1A_{1,1}+2w_2A_{1,2}+\\ldots+2w_nA_{1,n}\\\\\n",
    "&=2\\left(w_1A_{1,1}+w_2A_{1,2}+\\ldots+w_nA_{1,n}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The same holds for the partial derivative with respect to other weights $w_j$. This can therefore be expressed as the following matrix vector multiplication\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}w^\\top XX^\\top w=2XX^\\top w\n",
    "$$\n",
    "\n",
    "The full equation becomes the following\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla J(w)\n",
    "&=\\frac{1}{2m}\\left(2XX^\\top w-2XY^\\top\\right)\\\\\n",
    "&=\\frac{1}{m}\\left(XX^\\top w-XY^\\top\\right)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Setting this equation to $0$ and solving for $w$ becomes\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{1}{m}\\left(XX^\\top w-XY^\\top\\right)&=0\\\\\n",
    "    XX^\\top w-XY^\\top&=0\\\\\n",
    "    XX^\\top w&=XY^\\top\\\\\n",
    "    w&=\\left(XX^\\top\\right)^{-1}XY^\\top\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8d18373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11, 442), (1, 442))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = diabetes_X\n",
    "Y = diabetes_Y\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b430cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ne = np.dot(np.linalg.inv(np.dot(X, X.T)), np.dot(X, Y.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "806ce693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared errors:\n",
      "Normal equation : 2859.69\n",
      "Gradient descent: 2874.91\n"
     ]
    }
   ],
   "source": [
    "mse_ne = np.mean((np.dot(w_ne.T, X) - Y) ** 2)\n",
    "mse_gd = np.mean((np.dot(weights.T, X) - Y) ** 2)\n",
    "\n",
    "print('Mean squared errors:')\n",
    "print('Normal equation : {:.2f}'.format(mse_ne))\n",
    "print('Gradient descent: {:.2f}'.format(mse_gd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf726be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ -10.01219782],\n",
       "        [-239.81908937],\n",
       "        [ 519.83978679],\n",
       "        [ 324.39042769],\n",
       "        [-792.18416163],\n",
       "        [ 476.74583782],\n",
       "        [ 101.04457032],\n",
       "        [ 177.06417623],\n",
       "        [ 751.27932109],\n",
       "        [  67.62538639],\n",
       "        [ 152.13348416]]),\n",
       " array([[  -7.0341685 ],\n",
       "        [-236.60114314],\n",
       "        [ 527.60323623],\n",
       "        [ 321.83339165],\n",
       "        [-163.94305623],\n",
       "        [ -26.00582044],\n",
       "        [-174.92850824],\n",
       "        [ 105.19736381],\n",
       "        [ 514.96833878],\n",
       "        [  69.77383282],\n",
       "        [ 152.13348416]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_ne, weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
